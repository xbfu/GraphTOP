{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reviewer dFHj",
   "id": "76ebc8000edab6ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We thank the reviewer for providing valuable comments. We hope our point-by-point responses can fully address all your concerns.\n",
    "\n",
    "---\n",
    "- W1: It has been shown that a standard GNN (such as that in Eq. 1) cannot solve link prediction tasks (Zhang et al 2021). Therefore, using a standard GNN for \"pre-training, adaptation\" when pre-training is link prediction is not ideal, as it will not be able to perform the pre-training task well. I think you either should not consider link prediction as pre-training task (either remove it from the paper or move it to the appendix clarifying why theoretically is not a good strategy given the model) or change the GNN model, as shown in Bevilacqua et al., 2025.\n",
    "\n",
    "- ***R1***: Thanks for bringing this up. We would like to argue that designing effective pre-training strategies for training powerful GNNs is still an open question in graph learning.\n",
    "\n",
    "---\n",
    "\n",
    "- W2: The evaluation section is weak. The considered datasets are small and outdated. I would recommend to include the ogb datasets (arxiv, proteins, products).\n",
    "- ***R2***: Thanks for bringing this up. We conduct experiments on ogbn-arxiv and provide the results under GraphCL and LP-GraphPrompt as follows.\n",
    "\n",
    "---\n",
    "\n",
    "- Q1: Please connect your subgraph-constrained topology-oriented prompting with the line of work of Subgraph GNNs, which also represents related work. See for instance Southern et al., 2025 and all works therein.\n",
    "\n",
    "- A1: Thanks for bringing this up. We will include related works of subgraph GNNs in the revised version of our paper."
   ],
   "id": "374c2e89cba9caac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reviewer pf7m",
   "id": "4b8cab77ac104b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We thank the reviewer for providing valuable comments. We hope our point-by-point responses can fully address all your concerns.\n",
    "\n",
    "---\n",
    "- W1: Missing related work - This work (https://arxiv.org/pdf/2503.00750) is graph prompting from edge perspective and should be somewhat related.\n",
    "- R1: Thanks for getting our attention for this excellent work. We will include it in the revised version of our paper.\n",
    "\n",
    "---\n",
    "\n",
    "- W2: Writing could be improved - the current writing is not easy-to-follow, the paper is presented in a dense and notation-heavy manner, adding some illustrative figure could help.\n",
    "- R2: Thanks for pointing it out. We will try to add some illustrative figures to make the paper more readable.\n",
    "\n",
    "---\n",
    "\n",
    "- Q1: Is this topology-based prompt compatible with existing feature-based prompt? any chance they could be combined to make further improvement?\n",
    "- A1: Thanks for bringing this up. Yes, they are compatible and could be combined. It is definitely a promising scheme to combine for further improvement. We will explore this in our future work.\n",
    "\n",
    "---\n",
    "\n",
    "- Q2: Is there any visualized example to illustrate a subgraph before/after re-wiring?\n",
    "- A2: Thanks for pointing it out. Since we are not allowed to provide figures during rebuttal, we will perform the case study in the revised version of our paper."
   ],
   "id": "a1e8faea34dc317a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reviewer JEMG",
   "id": "1b9dc898ae0c1705"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We thank the reviewer for providing valuable comments. We hope our point-by-point responses can fully address all your concerns.\n",
    "\n",
    "---\n",
    "- C1&Q1: Assuming that the topological challenge is reasonable enough, in many graphs (such as chemical molecules and transportation networks), the existence of edges is determined a priori, and modifying the adjacency matrix may introduce unreasonable pseudo-structures.\n",
    "- R1: Thanks for pointing out this insightful question. We agree that topology-oriented prompting will learn the modified adjacency matrix involving pseudo-structures. However, we would like to emphasize that the goal of topology-oriented prompting is to enhance model utility of a pre-trained GNN model for downstream tasks. In fact, the modified adjacency matrix will be more suitable for the pre-trained GNN model, although it may be different from the original one. Similarly, the modified node features by feature-oriented prompting may also include pseudo-features (e.g., the age feature is modified to a negative value). We thank the reviewer for bringing up how to understand the modified data by prompting methods, which is very intriguing but has not been well investigated yet. We think it is also an open question in other fields, i.e., the meaning of soft tokens in NLP [1] and the meaning of soft patches in CV [2].\n",
    "\n",
    "---\n",
    "- C2: The experimental tasks are single and all focus on node classification.\n",
    "- R2: Thanks for pointing it out. We would like to clarify that this paper mainly focuses on node classification as the downstream task. As indicated in Appendix D, the proposed method can be extended to graph-level tasks, which are kept as our future work.\n",
    "\n",
    "---\n",
    "- C3&Q2: Some similar methods may be considered as baselines to compare, or for moderate discussion to distinguish them, such as Graph Structure Learning methods.\n",
    "- R3: Thanks for bringing this up. We agree that graph structure learning also learns to modify graph structures. However, graph structure learning follows an end-to-end manner by training GNN models via supervised learning. This assumption is fundamentally different from graph prompting where GNN models are pre-trained via unsupervised learning and kept frozen during adaptation. Therefore, we argue that graph structure learning methods are not appropriate baselines in the experiments. We will add the discussion in the revised version of our paper.\n",
    "\n",
    "---\n",
    "- C4&Q3: Regarding the experiments with different numbers of shots, some comparisons are missing, and the experiments in the appendix did not control the pre-training method to remain consistent.\n",
    "- R4: Thanks for bringing this up. We would like to clarify that the experiments with different numbers of shots do not aim to compare the performance of one graph prompting method with different numbers of shots. Instead, we are comparing the performance of different graph prompting methods in the same condition (the same pre-trained GNN model and the same number of shots). We use different pre-training strategies here because we hope to provide results in more conditions. We will clarify this in the revised version of our paper."
   ],
   "id": "a6ab545a880f6380"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reviewer 6YGZ",
   "id": "c65c71ae84ff3834"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We thank the reviewer for providing valuable comments. We hope our point-by-point responses can fully address all your concerns.\n",
    "\n",
    "---\n",
    "- W1&Q1: How does GraphTOP perform on synthetic datasets? When scaling to larger graphs (exceeding 100,000 nodes), can it maintain a reasonable balance between model effectiveness and computational efficiency?\n",
    "- **A1**: Thanks for bringing this up. We conduct experiments on a synthetic graph based on the CSBM and provide the results under SimGRACE and LP-GPPT as follows.\n",
    "\n",
    "Additionally, we conduct experiments on ogbn-arxiv with ~170K nodes and provide the results under GraphCL and LP-GraphPrompt as follows.\n",
    "\n",
    "---\n",
    "- W2&Q2: The proof in Section 5.2 regarding edge rewiring's effect on inter-class distance expansion relies on linear additivity assumptions. While the authors have noted potential issues under nonlinear conditions with dynamic edge weights, could supplementary theoretical or experimental analysis be provided for nonlinear GCNs (e.g., GAT)?\n",
    "- A2: Thanks for bringing this up. Our theoretical analysis follows classical studies [1, 2] analyzing linear separability under GNNs using linear GCNs because GCNs are the most popular GNNs in graph learning. We will analyze other complex GNNs (e.g., GAT) in our future work.\n",
    "\n",
    "---\n",
    "- Q3: The selection of parameters $\\lambda_1$ and $\\lambda_2$ shows significant impact on results. Could more robust optimization strategies be proposed?\n",
    "- A3: Thanks for bringing this up. In our experiments, we conduct grid search on different values of $\\lambda_1$ and $\\lambda_2$. Their optimal values could vary across datasets and pre-training methods. More robust optimization strategies, such as Bayesian Optimization and Hyperband, could be used for hyperparameter selection.\n",
    "\n",
    "[1] Ma, Yao, et al. \"Is Homophily a Necessity for Graph Neural Networks?.\" ICLR 2022.\n",
    "[2] Baranwal, Aseem, Kimon Fountoulakis, and Aukosh Jagannath. \"Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization.\" ICML 2021."
   ],
   "id": "6f4b75b915ca9cd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e754e47df8cb2c1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
